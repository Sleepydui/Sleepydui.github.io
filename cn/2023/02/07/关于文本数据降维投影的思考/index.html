<a name=top></a><!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Xinyue-page</title><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css><link href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css rel=stylesheet><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/bash.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js></script><script src=https://cdn.jsdelivr.net/npm/vega@5.17.0></script><script src=https://cdn.jsdelivr.net/npm/vega-lite@4.17.0></script><script src=https://cdn.jsdelivr.net/npm/vega-embed@6.12.2></script><script>hljs.initHighlightingOnLoad();</script><link rel=icon href=https://Sleepydui.github.io/image/cxy.png></head><body><div class=wrapper><header class=header><nav class=nav><a href=/ class=nav-logo><img src=/image/cxy.png width=70 height=40 alt=Hugo-ht></a><ul class=nav-links><li><a href=/cn/about/>关于</a></li><li><a href=/cn/posts/>日志</a></li><li><a href=/>English</a></li></ul></nav></header><main class=content role=main><div style=text-align:center><h1>关于文本数据降维投影的思考</h1><p>陈昕悦
/ 2023-02-07</p><hr></div><span class=article-toolbar><a href=https://github.com/Sleepydui/Sleepydui.github.io/edit/master/content/cn/posts/2023-02-07-dimensionality-reduction.md style=font-size:24px;color:#000 target=_blank><i class="fa fa-edit" aria-hidden=true title=编辑本页></i></a></span><div class="body-text list-text"><ul><li><p>文本数据是我在可视化研究中最常使用的数据类型</p><ul><li>俄乌新闻可视化画contour的时候我曾用过TF-IDF与PCA</li><li>但其实不明白背后的原理</li><li>而且contour的效果并不好（1）overlap太多（2）stopword并没有过滤掉太多没有实际意义的词导致聚类含义不明</li><li>在阅读wiki时，我发现lab的习惯是先用降维投影观察一下数据</li><li>如media bias （http://vis.pku.edu.cn/wiki/doku.php?id=visgroup:projects:biasvis:data）</li></ul></li><li><p>PCA（Principal Component Analysis）</p><ul><li>使用 PCA 算法的步骤如下：<ol><li>用TfidfVectorizer() 对输入的文本数据进行特征提取，生成特征矩阵vectors</li><li>然后，使用 PCA（Principal Component Analysis）算法对特征矩阵 vectors 进行降维，降维后的结果存储在 vectorspca 中。n_components 参数代表降维后的维数，即将特征矩阵降维为 2 维。</li></ol></li><li>PCA 算法是一种常用的降维算法，它的原理是在保证数据的总方差不变的情况下，对数据进行线性变换，使得变换后的数据具有最大的方差。这样，在降维的同时，尽量保留原始数据的信息。</li><li>因此，PCA 降维是通过找到数据的主成分，以尽可能保证数据总方差不变的情况下降低数据维数来实现降维的。</li></ul></li><li><p>T-SNE (t-Distributed Stochastic Neighbor Embedding)</p><ul><li>T-SNE是一种高维数据的降维算法，可以帮助我们将高维的数据降到低维，并且保持数据之间的相似性关系。</li><li>它的使用方法很简单，我们只需要提供原始的高维数据和降维后的维数，T-SNE 就可以计算出一个新的低维数据，并且用这个低维数据来可视化原始数据。</li><li>T-SNE 的一个重要依据是数据之间的相似性关系，它通过计算数据间的近邻关系来生成降维数据，从而可以更好地呈现数据的分布。<ol><li>T-SNE 计算相似性的原理基于两个假设：数据之间的相似性关系以及低维数据的分布更加集中。</li><li>在原始的高维数据中，T-SNE 计算出数据之间的相似性，它将相似的数据映射到近邻关系，而不相似的数据映射到远离关系。</li><li>然后，T-SNE 在低维数据中重新计算数据之间的相似性，使得相似的数据仍然是近邻关系，而不相似的数据仍然是远离关系。最终，T-SNE 将低维数据放到二维或三维平面上，以可视化方式呈现数据分布。</li><li>通过这样的过程，T-SNE 可以保持原始数据中数据之间的相似性关系，并且使得低维数据的分布更加集中，从而更好地呈现数据的分布情况。</li></ol></li><li>通常情况下，T-SNE 适用于处理非线性数据，因为它可以捕捉到数据之间的非线性关系。它在许多的数据分析场景中都有很好的应用，比如文本分类、图像分类、生物信息学等。</li></ul></li><li><p>SVD（Singular Value Decomposition）算法是一种数学分解方法，它可以将一个矩阵分解为三个矩阵的乘积，分别是左奇异矩阵、奇异值矩阵和右奇异矩阵。</p><ul><li>在机器学习和数据分析中，SVD 算法常用来降维，去噪和特征提取。例如，在推荐系统中，可以使用 SVD 算法来研究用户对物品的评分，以实现更准确的推荐。</li><li>使用 SVD 算法的步骤如下：<ol><li>导入必要的库，并载入数据集。</li><li>对数据集进行 SVD 分解。</li><li>应用分解结果，进行降维、去噪或特征提取。</li><li>对结果进行评估，并选择最优的降维结果。</li></ol></li><li>数学解释：一个矩阵 A 的 SVD 分解是将 A 分解为三个矩阵的乘积：A = UΣV^T，<ol><li>其中U 是一个左奇异矩阵，其列向量为 A 的左奇异向量。</li><li>Σ 是一个对角矩阵，其对角线上的元素是 A 的奇异值，即 A 的左奇异向量与右奇异向量的内积。</li><li>V^T 是 A 的右奇异矩阵，其行向量为 A 的右奇异向量。</li></ol></li><li>分解矩阵的作用：<ol><li>降维：SVD 可以用于降维，因为可以通过保留较少的奇异值和对应的左右奇异向量来近似矩阵。</li><li>对矩阵进行特征提取：通过 SVD 可以提取出矩阵的主要特征，对于数据挖掘和数据分析非常有用。</li><li>对矩阵进行矩阵运算：SVD 分解后的矩阵具有结构性，可以用于矩阵的快速运算。</li><li>对矩阵进行压缩：SVD 可以用于数据压缩，因为可以通过保留较少的奇异值和对应的左右奇异向量来近似矩阵。</li></ol></li></ul></li><li><p>文本特征提取（BERT 和 TF-IDF 是不同的文本特征提取方法，两者生成的特征矩阵不同）</p><ul><li>BERT：BERT 是一种 Transformer-based 模型，是一种预训练的深度双向语言模型。它的特征提取方法是通过语义理解文本的方式生成的，并不是通过词袋模型的方式生成的。BERT 通过全词覆盖来理解词语的语境，这些理解被转化为对词语的向量表示。该模型的输入是一个文本的句子，它的输出是一个固定长度的向量，这个向量可以看作是这个句子的特征向量。</li><li>TF-IDF：TF-IDF 是一种常用的文本特征提取方法，它通过词频和逆文档频率的方式生成的。词频表示一个词语在文档中出现的频率，逆文档频率是一个词语在整个语料库中的出现频率的倒数。TF-IDF 特征矩阵的每一列对应一个词语，每一行对应一个文档，每一个元素是一个文档中一个词语的 TF-IDF 值。</li><li>生成的特征矩阵和原始文本数据是通过向量表示相对应的。对于 BERT，特征向量是语义表示；对于 TF-IDF，特征矩阵是文档中词语出现的频率和在整个语料库中的重要性</li></ul></li></ul><p style=color:#777>最后一次修改于 2023-02-07</p></div><a href=#top><i class="fa fa-chevron-up" style=font-size:30px;color:#000></i></a></main><footer class=footer><script type=text/javascript src=/js/math-code.js></script><script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script><script type=text/javascript src=/js/center-img.js></script><ul class=footer-links><li><a href=/cn/posts/index.xml type=application/rss+xml title="RSS feed">订阅</a></li><li><a href=http://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh target=_blank>版权
<i class="fa fa-cc" aria-hidden=true title="Attribution-NonCommercial-ShareAlike 4.0 International"></i></a></li></ul><div class=copyright-text>©
陈昕悦
2023-∞</div></footer>